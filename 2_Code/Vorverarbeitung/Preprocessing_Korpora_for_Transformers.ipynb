{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Corpora\n",
    "\n",
    "<hr>\n",
    "\n",
    "This notebook is used to prepare the corpora for sentiment analysis. \n",
    "- Stop Word Removel\n",
    "- Translate emoticons\n",
    "- Stemming\n",
    "- Removel of Usernames/URL/Hashtags\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Source:\n",
    "\n",
    "[1] Parveen, H., & Pandey, S. (2016, July). Sentiment analysis on Twitter Data-set using Naive Bayes algorithm. In 2016 2nd international conference on applied and theoretical computing and communication technology (ICATCCT) (pp. 416-419). IEEE. <br>\n",
    "[2] Basarslan, M. S., & Kayaalp, F. (2020). Sentiment Analysis with Machine Learning Methods on Social Media. <br>\n",
    "[3] Jagdale, R. S., Shirsat, V. S., & Deshmukh, S. N. (2019). Sentiment analysis on product reviews using machine learning techniques. In Cognitive Informatics and Soft Computing (pp. 639-647). Springer, Singapore.<br>\n",
    "\n",
    "## Libaries \n",
    "[4] Emoji https://pypi.org/project/emoji/ <br>\n",
    "[5] Pandas https://pandas.pydata.org/ <br>\n",
    "[6] nltk https://www.nltk.org/ <br>\n",
    "[7] Tabulate https://pypi.org/project/tabulate/ <br>\n",
    "[8] Re https://docs.python.org/3/library/re.html <br>\n",
    "[9] os https://docs.python.org/3/library/os.html <br>\n",
    "[10] Time https://docs.python.org/3/library/time.html <br>\n",
    "[11] cleantext https://pypi.org/project/clean-text/ <br>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from nicerPrint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import import_ipynb\n",
    "import nicerPrint as nicer\n",
    "import emoji\n",
    "import time\n",
    "from emoji import EMOJI_DATA\n",
    "\n",
    "from cleantext import clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "1. Stop Word Removel [2]\n",
    "\n",
    "- Loads german stop words from nltk\n",
    "- fill NaN cells with an empty string\n",
    "- remove stop words and ~http string\n",
    "- remove Speaker from beginning of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stopWord(df):   \n",
    "    df[\"text\"]=df[\"text\"].fillna(\"\")\n",
    "    df[\"preprocessedData\"]=df[\"text\"]\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"~http\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='^[A-Z][a-z]{1,}[:]', value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Emoticons [1]\n",
    "- translateEmot replace old emojis with their meaning \n",
    "- translateEmoji uses [4] to translate newer emojis to words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateEmot(df):\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:=][-]?[D]\", value=\"lachen\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[d)]\", value=\"lächeln\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[;][-]?[)]\", value=\"zwinkern\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[oO]\", value=\"überrascht\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[pP]\", value=\"verspielt\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[([]\", value=\"traurig\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[\\/]\", value=\"verwirred\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:*>]{3}\", value=\"peinlich\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[B8][-]?[|]\", value=\"cool\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[xX][-]?[(]\", value=\"wütend\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[xX]\", value=\"Liebe\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[xX][-]?[)|]\", value=\"müde\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][,`'][(]\", value=\"weinen\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[A-Z]{3,}[\\s]?[:]?\", value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translateEmoji(sentence):\n",
    "    wordList=sentence.split()\n",
    "    newWordList=[]\n",
    "    \n",
    "    for word in wordList:\n",
    "        if emoji.is_emoji(word) is True:\n",
    "            emojiTrans=emoji.demojize(word,language=\"de\")\n",
    "            withoutDots=re.sub(\":\",\"\",emojiTrans)\n",
    "            withoutUnderscore=re.sub(\"_\",\" \",withoutDots)\n",
    "            newWordList.append(withoutUnderscore)\n",
    "        else:\n",
    "            newWordList.append(word)  \n",
    "            \n",
    "    newSentence=\" \".join(newWordList)\n",
    "    return newSentence\n",
    "\n",
    "def translateEmoj(df):\n",
    "    column= df[\"preprocessedData\"]\n",
    "    for index in range(len(column.values)):\n",
    "        textElement=column.values[index]\n",
    "        df.at[index,\"preprocessedData\"]=translateEmoji(textElement)\n",
    "\n",
    "#Print Output:\n",
    "#print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removel of Usernames/URLS/Hashtags\n",
    "- removes links\n",
    "- removes hashtags\n",
    "- removes usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUser(df):\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"http\\S+\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"\\B(\\#[a-zöüäA-ZÖÜÄ0-9_]+\\b)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"\\B(\\@[a-zöüäA-ZÖÜÄ0-9_]+\\b)\", value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Treatment for corpora\n",
    "- removes special cases «» and \" and ...\n",
    "- removes RT or Re from twitter\n",
    "- removes left over emojis\n",
    "- removes every special character from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def specialProcessing(df):\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"[«»]\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(\\\"RT)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(Re:)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(\\\"RT :)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(RT :)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(RT)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r'[\\\"\\'\\`\\´]{3,}', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='(\"Rt :)', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='(Rt :)', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='(Rt)', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='[\\u0022\\u00A8\\u0027\\u0060\\u00B4\\u2018\\u2019\\u201C\\u201D]{3,}', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r'[.]{3,}', value=\"\", regex=True, inplace=True)\n",
    "\n",
    "    column= df[\"preprocessedData\"]\n",
    "    for index in range(len(column.values)):\n",
    "        textElement=column.values[index]\n",
    "        clean(textElement, no_emoji=True)\n",
    "    \n",
    "## Final Remove\n",
    "    df[\"preprocessedData\"].replace(to_replace='[^a-zA-Z\\.\\,\\!\\?\\&\\; ÖÄÜöäü0-9]', value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new data in .tsv file\n",
    "\n",
    "Special cases:\n",
    "- sentiment_binary\n",
    "- summary\n",
    "- sentiment_4agree\n",
    "- sentiment_3agree\n",
    "- sentiment_2agree\n",
    "- no additional line\n",
    "\n",
    "- adds special columns if there excists \n",
    "- adds id, preprocessed to the new df\n",
    "- removes the neutral entry for a binary sentiment (in a few corpora the sentiment neutral is Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addNewColumns(df,preprocessedDf):\n",
    "    if \"sentiment\" in df.columns:\n",
    "        preprocessedDf[\"labels\"]=df.sentiment\n",
    "    \n",
    "    \n",
    "        \n",
    "def saveTernary(df,fileName,startTime):\n",
    "    preprocessedDf=pd.DataFrame(columns=[\"id\",\"text\"])\n",
    "    preprocessedDf[\"id\"]=df.id\n",
    "    preprocessedDf[\"text\"]=df.preprocessedData\n",
    "    \n",
    "    addNewColumns(df,preprocessedDf)\n",
    "    \n",
    "    #preprocessedDf=preprocessedDf[preprocessedDf[\"labels\"].str.contains(\"mixed\")==False]\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"mixed\", value=5, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"negativ\", value=1, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"positiv\", value=0, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"negative\", value=1, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"positive\", value=0, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"neutral\", value=2, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"Negative\", value=1, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"Positive\", value=0, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"Neutral\", value=2, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"Negativ\", value=1, regex=True, inplace=True)\n",
    "    preprocessedDf[\"labels\"].replace(to_replace=\"Positiv\", value=0, regex=True, inplace=True)\n",
    "    preprocessedDf=preprocessedDf[preprocessedDf.labels !=5]\n",
    "    \n",
    "    preprocessedDf = preprocessedDf[preprocessedDf[\"text\"].notna()]\n",
    "    fileNameNew=fileName+\"_Preprocessed_ternary_Transformer.tsv\"\n",
    "    preprocessedDf.to_csv(fileNameNew, sep=\"\\t\",index=False)\n",
    "    \n",
    "    endTime=time.time()\n",
    "    print(endTime-startTime,\"Seconds\")\n",
    "    nicer.printS(\"End\",\"red\")\n",
    "    \n",
    "    saveBinary(preprocessedDf,fileName)\n",
    "\n",
    "    \n",
    "def saveBinary(preprocessedDf,fileName):\n",
    "    preprocessedDf=preprocessedDf[preprocessedDf.labels !=2]\n",
    "\n",
    "    fileNameNew=fileName+\"_Preprocessed_binary_Transformer.tsv\"\n",
    "    preprocessedDf.to_csv(fileNameNew, sep=\"\\t\",index=False)\n",
    "    #print(tabulate(newDf, headers='firstrow',showindex=\"false\", tablefmt='tsv'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpora/ Main function\n",
    "\n",
    "- Loads the corpora\n",
    "- split file type from file name\n",
    "- calls other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart\u001b[0m\n",
      "FileName:  SM02_potts \n",
      "\n",
      "1.5243475437164307 Seconds\n",
      "\u001b[31mEnd\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    startTime=time.time()\n",
    "    \n",
    "    file=\"../../Corpora/Normalized/SM02_potts.tsv\"\n",
    "    path,fileName =os.path.split(file)\n",
    "    fileName= re.sub(\".tsv\",\"\",fileName)\n",
    "    \n",
    "    nicer.setState(True)\n",
    "    nicer.printS(\"Start\",\"red\")\n",
    "    \n",
    "    df = pd.read_csv(file , sep=\"\\t\")\n",
    "    print(\"FileName: \",fileName, \"\\n\")\n",
    "    \n",
    "    stopWord(df)\n",
    "    translateEmot(df)\n",
    "    translateEmoj(df)\n",
    "    removeUser(df)\n",
    "    specialProcessing(df)\n",
    "    saveTernary(df,fileName,startTime)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                               text  labels\n",
      "262230  262230             Super Spielnur leider sehr donutlastig       0\n",
      "88634    88634                        Das Programm ist sehr gut .       0\n",
      "171339  171339  Eigentlich ist es das beste Navi. Deswegen hab...       0\n",
      "123422  123422  Man bekommt auf die sekunde genau eine Nachric...       0\n",
      "1100      1100  Sehr gut! Läuft bei mir problemlos aus Samsung...       0\n",
      "...        ...                                                ...     ...\n",
      "553605  553605  Bei mir kommt immer Werbung von Bild irgendwo ...       2\n",
      "636193  636193  bringt leider nicht die erhoffte Verbesserung....       2\n",
      "142586  142586           Kann seit kurzem keine Fotos runterladen       2\n",
      "30092    30092  Ich finde freeletics generell extrem gut, woll...       2\n",
      "559598  559598   bis das letzte Update kam! Dieses lie sich ni...       2\n",
      "\n",
      "[70002 rows x 3 columns]\n",
      "0    23334\n",
      "1    23334\n",
      "2    23334\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_id = \"../../Corpora/Preprocessed/Ternary/RE02_scare_Preprocessed_ternary_balanced.tsv\"\n",
    "file_new = \"RE02_scare_all_Preprocessed_ternary_Transformer.tsv\"\n",
    "\n",
    "idDf = pd.read_csv(file_id , sep=\"\\t\")\n",
    "textDf=pd.read_csv(file_new,sep=\"\\t\")\n",
    "\n",
    "\n",
    "allDataList=[]\n",
    "for ids in range(len(idDf[\"id\"])):\n",
    "    number=idDf[\"id\"][ids]\n",
    "    allDataList.append(textDf.iloc[number])\n",
    "    \n",
    "updateTextDf = pd.DataFrame(allDataList,columns=[\"id\",\"text\",\"labels\"])\n",
    "print(updateTextDf)\n",
    "count=updateTextDf[\"labels\"].value_counts()\n",
    "print(count)\n",
    "#updateTextDf.to_csv(\"RE02_scare_Preprocessed_ternary_Transformer_balanced.tsv\", sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

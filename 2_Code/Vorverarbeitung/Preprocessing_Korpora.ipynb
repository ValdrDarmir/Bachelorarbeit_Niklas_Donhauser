{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Corpora\n",
    "\n",
    "<hr>\n",
    "\n",
    "This notebook is used to prepare the corpora for sentiment analysis. \n",
    "- Stop Word Removel\n",
    "- Translate emoticons\n",
    "- Stemming\n",
    "- Removel of Usernames/URL/Hashtags\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Source:\n",
    "\n",
    "[1] Parveen, H., & Pandey, S. (2016, July). Sentiment analysis on Twitter Data-set using Naive Bayes algorithm. In 2016 2nd international conference on applied and theoretical computing and communication technology (ICATCCT) (pp. 416-419). IEEE. <br>\n",
    "[2] Basarslan, M. S., & Kayaalp, F. (2020). Sentiment Analysis with Machine Learning Methods on Social Media. <br>\n",
    "[3] Jagdale, R. S., Shirsat, V. S., & Deshmukh, S. N. (2019). Sentiment analysis on product reviews using machine learning techniques. In Cognitive Informatics and Soft Computing (pp. 639-647). Springer, Singapore.<br>\n",
    "\n",
    "## Libaries \n",
    "[4] Emoji https://pypi.org/project/emoji/ <br>\n",
    "[5] Pandas https://pandas.pydata.org/ <br>\n",
    "[6] nltk https://www.nltk.org/ <br>\n",
    "[7] Tabulate https://pypi.org/project/tabulate/ <br>\n",
    "[8] Re https://docs.python.org/3/library/re.html <br>\n",
    "[9] os https://docs.python.org/3/library/os.html <br>\n",
    "[10] Time https://docs.python.org/3/library/time.html <br>\n",
    "[11] cleantext https://pypi.org/project/clean-text/ <br>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text[gpl]==0.4.0\n",
      "  Using cached clean_text-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.8/dist-packages (from clean-text[gpl]==0.4.0) (6.1.1)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (from clean-text[gpl]==0.4.0) (1.7.0)\n",
      "Requirement already satisfied: unidecode<2.0.0,>=1.1.1; extra == \"gpl\" in /usr/local/lib/python3.8/dist-packages (from clean-text[gpl]==0.4.0) (1.3.6)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]==0.4.0) (0.2.5)\n",
      "Installing collected packages: clean-text\n",
      "  Attempting uninstall: clean-text\n",
      "    Found existing installation: clean-text 0.6.0\n",
      "    Uninstalling clean-text-0.6.0:\n",
      "      Successfully uninstalled clean-text-0.6.0\n",
      "Successfully installed clean-text-0.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install clean-text[gpl]==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from nicerPrint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import import_ipynb\n",
    "import nicerPrint as nicer\n",
    "import emoji\n",
    "import time\n",
    "from emoji import EMOJI_DATA\n",
    "\n",
    "from cleantext import clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "1. Stop Word Removel [2]\n",
    "\n",
    "- Loads german stop words from nltk\n",
    "- fill NaN cells with an empty string\n",
    "- remove stop words and ~http string\n",
    "- remove Speaker from beginning of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stopWord(df):   \n",
    "    df[\"text\"]=df[\"text\"].fillna(\"\")\n",
    "    stopWords = stopwords.words(\"german\")\n",
    "    df[\"preprocessedData\"]=df[\"text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopWords)]))\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"~http\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='^[A-Z][a-z]{1,}[:]', value=\"\", regex=True, inplace=True)\n",
    "    \n",
    "\n",
    "## First Time: Download Stop Words    \n",
    "#stopWordsList = nltk.download(\"stopwords\")\n",
    "\n",
    "## Check output:\n",
    "#print(df[\"preprocessedData\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Translate Emoticons [1]\n",
    "- translateEmot replace old emojis with their meaning \n",
    "- translateEmoji uses [4] to translate newer emojis to words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateEmot(df):\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:=][-]?[D]\", value=\"lachen\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[d)]\", value=\"lächeln\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[;][-]?[)]\", value=\"zwinkern\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[oO]\", value=\"überrascht\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[pP]\", value=\"verspielt\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[([]\", value=\"traurig\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[\\/]\", value=\"verwirrt\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:*>]{3}\", value=\"peinlich\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[B8][-]?[|]\", value=\"cool\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[xX][-]?[(]\", value=\"wütend\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][-]?[xX]\", value=\"Liebe\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[xX][-]?[)|]\", value=\"müde\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[:][,`'][(]\", value=\"weinen\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"[A-Z]{3,}[\\s]?[:]?\", value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateEmoji(sentence):\n",
    "    wordList=sentence.split()\n",
    "    newWordList=[]\n",
    "    \n",
    "    for word in wordList:\n",
    "        if emoji.is_emoji(word) is True:\n",
    "            emojiTrans=emoji.demojize(word,language=\"de\")\n",
    "            withoutDots=re.sub(\":\",\"\",emojiTrans)\n",
    "            withoutUnderscore=re.sub(\"_\",\" \",withoutDots)\n",
    "            newWordList.append(withoutUnderscore)\n",
    "        else:\n",
    "            newWordList.append(word)  \n",
    "            \n",
    "    newSentence=\" \".join(newWordList)\n",
    "    return newSentence\n",
    "\n",
    "def translateEmoj(df):\n",
    "    column= df[\"preprocessedData\"]\n",
    "    for index in range(len(column.values)):\n",
    "        textElement=column.values[index]\n",
    "        df.at[index,\"preprocessedData\"]=translateEmoji(textElement)\n",
    "\n",
    "#Print Output:\n",
    "#print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Removel of Usernames/URLS/Hashtags\n",
    "- removes links\n",
    "- removes hashtags\n",
    "- removes usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUser(df):\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"http\\S+\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"\\B(\\#[a-zöüäA-ZÖÜÄ0-9_]+\\b)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r\"\\B(\\@[a-zöüäA-ZÖÜÄ0-9_]+\\b)\", value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Stemming [3]\n",
    "\n",
    "- uses snowball stemmer for the german language\n",
    "- before stemming check capitalization from word and keep capital letter if true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    snow=SnowballStemmer(language=\"german\")\n",
    "    column= df[\"preprocessedData\"]\n",
    "    \n",
    "    for index in range(len(column.values)):\n",
    "        textElement=column.values[index]\n",
    "        df.at[index,\"preprocessedData\"]=stemWord(textElement,snow)\n",
    "        \n",
    "def stemWord(sentence,snow):\n",
    "    wordList= sentence.split()\n",
    "    newWordList=[]\n",
    "    \n",
    "    for word in wordList:\n",
    "        stemWord= snow.stem(word)\n",
    "        if (word[0].isupper()):\n",
    "            stemWord= snow.stem(word)\n",
    "            stemWord=stemWord.capitalize()  \n",
    "        newWordList.append(stemWord)\n",
    "        \n",
    "    newSentence=\" \".join(newWordList)\n",
    "    #print(newSentence)\n",
    "    return newSentence\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Treatment for corpora\n",
    "- removes special cases «» and \" and ...\n",
    "- removes RT or Re from twitter\n",
    "- removes left over emojis\n",
    "- removes every special character from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def specialProcessing(df):\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"[«»]\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(\\\"RT)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(Re:)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(\\\"RT :)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(RT :)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=\"(RT)\", value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r'[\\\"\\'\\`\\´]{3,}', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='(\"Rt :)', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='(Rt :)', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='(Rt)', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace='[\\u0022\\u00A8\\u0027\\u0060\\u00B4\\u2018\\u2019\\u201C\\u201D]{3,}', value=\"\", regex=True, inplace=True)\n",
    "    df[\"preprocessedData\"].replace(to_replace=r'[.]{3,}', value=\"\", regex=True, inplace=True)\n",
    "\n",
    "    column= df[\"preprocessedData\"]\n",
    "    for index in range(len(column.values)):\n",
    "        textElement=column.values[index]\n",
    "        clean(textElement, no_emoji=True)\n",
    "    \n",
    "## Final Remove\n",
    "    df[\"preprocessedData\"].replace(to_replace='[^a-zA-Z\\.\\,\\!\\?\\&\\; ÖÄÜöäü0-9]', value=\"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new data in .tsv file\n",
    "\n",
    "Special cases:\n",
    "- sentiment_binary\n",
    "- summary\n",
    "- sentiment_4agree\n",
    "- sentiment_3agree\n",
    "- sentiment_2agree\n",
    "- no additional line\n",
    "\n",
    "- adds special columns if there excists \n",
    "- adds id, preprocessed to the new df\n",
    "- removes the neutral entry for a binary sentiment (in a few corpora the sentiment neutral is Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addNewColumns(df,preprocessedDf):\n",
    "    if \"sentiment\" in df.columns:\n",
    "        preprocessedDf[\"sentiment\"]=df.sentiment\n",
    "        \n",
    "    if \"sentiment_4agree\" in df.columns:\n",
    "        preprocessedDf[\"sentiment_4agree\"]=df.sentiment_4agree\n",
    "        \n",
    "    if \"sentiment_3agree\" in df.columns:\n",
    "        preprocessedDf[\"sentiment_3agree\"]=df.sentiment_3agree\n",
    "        preprocessedDf[\"sentiment_3agree\"]=preprocessedDf[\"sentiment_3agree\"].str.lower()\n",
    "        \n",
    "    if \"sentiment_2agree\" in df.columns:\n",
    "        preprocessedDf[\"sentiment_2agree\"]=df.sentiment_2agree\n",
    "        \n",
    "    if \"sentiment_binary\" in df.columns:\n",
    "        preprocessedDf[\"sentiment_binary\"]=df.sentiment_binary\n",
    "        \n",
    "    if \"summary\" in df.columns:\n",
    "        preprocessedDf[\"summary\"]=df.summary\n",
    "    \n",
    "def saveTernary(df,fileName,startTime):\n",
    "    preprocessedDf=pd.DataFrame(columns=[\"id\",\"preprocessedData\"])\n",
    "    preprocessedDf[\"id\"]=df.id\n",
    "    preprocessedDf[\"preprocessedData\"]=df.preprocessedData\n",
    "    \n",
    "    addNewColumns(df,preprocessedDf)\n",
    "    fileNameNew=fileName+\"_Preprocessed_ternary.tsv\"\n",
    "    preprocessedDf.to_csv(fileNameNew, sep=\"\\t\",index=False)\n",
    "    \n",
    "    endTime=time.time()\n",
    "    print(endTime-startTime,\"Seconds\")\n",
    "    nicer.printS(\"End\",\"red\")\n",
    "    \n",
    "    saveBinary(preprocessedDf,fileName)\n",
    "\n",
    "    \n",
    "def saveBinary(preprocessedDf,fileName):\n",
    "    preprocessedDf=preprocessedDf[preprocessedDf[\"sentiment\"].str.contains(\"neutral\")==False]\n",
    "\n",
    "    fileNameNew=fileName+\"_Preprocessed_binary.tsv\"\n",
    "    preprocessedDf.to_csv(fileNameNew, sep=\"\\t\",index=False)\n",
    "    #print(tabulate(newDf, headers='firstrow',showindex=\"false\", tablefmt='tsv'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpora/ Main function\n",
    "\n",
    "- Loads the corpora\n",
    "- split file type from file name\n",
    "- calls other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart\u001b[0m\n",
      "FileName:  RE05_amazonreviews_11 \n",
      "\n",
      "19.816954135894775 Seconds\n",
      "\u001b[31mEnd\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    startTime=time.time()\n",
    "    \n",
    "    file=\"../../Corpora/Normalized/RE05_amazonreviews_11.tsv\"\n",
    "    path,fileName =os.path.split(file)\n",
    "    fileName= re.sub(\".tsv\",\"\",fileName)\n",
    "    \n",
    "    nicer.setState(True)\n",
    "    nicer.printS(\"Start\",\"red\")\n",
    "    \n",
    "    df = pd.read_csv(file , sep=\"\\t\")\n",
    "    print(\"FileName: \",fileName, \"\\n\")\n",
    "    \n",
    "    stopWord(df)\n",
    "    translateEmot(df)\n",
    "    translateEmoj(df)\n",
    "    removeUser(df)\n",
    "    stemming(df)\n",
    "    specialProcessing(df)\n",
    "    saveTernary(df,fileName,startTime)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

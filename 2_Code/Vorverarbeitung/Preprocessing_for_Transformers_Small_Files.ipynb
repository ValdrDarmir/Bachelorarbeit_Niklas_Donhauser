{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Transformers\n",
    "Name: Niklas Donhauser\n",
    "<hr>\n",
    "\n",
    "In this notebook, text without preprocessing is loaded with the ids from the preprocessed corpora which are used in the svm classification task. Bigger corpora are getting transformed in the \"Preprocessing_for_Transformers_Big_Files\" notebook, because of performance issues. <br>\n",
    "\n",
    "### Information about the different functions used here: <br>\n",
    "[1] **loadFiles:** <br>\n",
    "All paths of the different corpora are stored in lists here <br>\n",
    "[2] **extractId:** <br>\n",
    "Extract the used Ids from the PP corpora and stores them in a list <br>\n",
    "[3] **extractText:** <br>\n",
    "Get the text data from the raw data corpus with the ids extract from the previous function <br>\n",
    "[4] **insertNewData:** <br>\n",
    "Insert the new text data in a dataframe and transfer this to a list. Also add the data to the result dataframe <br>\n",
    "[5] **updateDataframe:** <br>\n",
    "Updates the dataframe, changes column names and replace label names <br>\n",
    "[6] **main:** <br>\n",
    "Loads, transform and save the data <br>\n",
    "\n",
    "**Source**\n",
    "\n",
    "[1] random https://docs.python.org/3/library/random.html <br>\n",
    "[2] os https://docs.python.org/3/library/os.html <br>\n",
    "[3] pandas https://pandas.pydata.org/ <br>\n",
    "[4] numpy https://numpy.org/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFiles():\n",
    "    file_binary=[\"../../Corpora/Preprocessed/Binary/LT01_gnd_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/LT02_speechLessing_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/LT03_historicplays_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/MI01_mlsa_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/MI02_germeval_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/MI03_corpusRauh_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/NA01_gersen_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/NA02_gerom_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/NA03_ompc_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/RE01_usage_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/RE03_critics_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/SM01_sb10k_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/SM02_potts_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/SM03_multiSe_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/SM04_gertwittersent_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/SM05_ironycorpus_Preprocessed_binary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Binary/SM06_celeb_Preprocessed_binary.tsv\"\n",
    "            ]\n",
    "    file_binary_raw=[\"../../Corpora/Normalized/LT01_gnd.tsv\",\n",
    "            \"../../Corpora/Normalized/LT02_speechLessing.tsv\",\n",
    "            \"../../Corpora/Normalized/LT03_historicplays.tsv\",\n",
    "            \"../../Corpora/Normalized/MI01_mlsa.tsv\",\n",
    "            \"../../Corpora/Normalized/MI02_germeval.tsv\",\n",
    "            \"../../Corpora/Normalized/MI03_corpusRauh.tsv\",\n",
    "            \"../../Corpora/Normalized/NA01_gersen.tsv\",\n",
    "            \"../../Corpora/Normalized/NA02_gerom.tsv\",\n",
    "            \"../../Corpora/Normalized/NA03_ompc.tsv\",\n",
    "            \"../../Corpora/Normalized/RE01_usage.tsv\",\n",
    "            \"../../Corpora/Normalized/RE03_critics.tsv\",     \n",
    "            \"../../Corpora/Normalized/SM01_sb10k.tsv\",\n",
    "            \"../../Corpora/Normalized/SM02_potts.tsv\",\n",
    "            \"../../Corpora/Normalized/SM03_multiSe.tsv\",\n",
    "            \"../../Corpora/Normalized/SM04_gertwittersent.tsv\",\n",
    "            \"../../Corpora/Normalized/SM05_ironycorpus.tsv\",\n",
    "            \"../../Corpora/Normalized/SM06_celeb.tsv\"\n",
    "            ]\n",
    "    file_ternary=[\"../../Corpora/Preprocessed/Ternary/LT01_gnd_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/LT02_speechLessing_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/MI01_mlsa_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/MI02_germeval_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/MI03_corpusRauh_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/NA01_gersen_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/NA02_gerom_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/NA03_ompc_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/RE01_usage_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/RE03_critics_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/SM01_sb10k_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/SM02_potts_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/SM03_multiSe_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/SM04_gertwittersent_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/SM05_ironycorpus_Preprocessed_ternary.tsv\",\n",
    "            \"../../Corpora/Preprocessed/Ternary/SM06_celeb_Preprocessed_ternary.tsv\"\n",
    "            ]\n",
    "    file_ternary_raw=[\"../../Corpora/Normalized/LT01_gnd.tsv\",\n",
    "            \"../../Corpora/Normalized/LT02_speechLessing.tsv\",\n",
    "            \"../../Corpora/Normalized/MI01_mlsa.tsv\",\n",
    "            \"../../Corpora/Normalized/MI02_germeval.tsv\",\n",
    "            \"../../Corpora/Normalized/MI03_corpusRauh.tsv\",\n",
    "            \"../../Corpora/Normalized/NA01_gersen.tsv\",\n",
    "            \"../../Corpora/Normalized/NA02_gerom.tsv\",\n",
    "            \"../../Corpora/Normalized/NA03_ompc.tsv\",\n",
    "            \"../../Corpora/Normalized/RE01_usage.tsv\",\n",
    "            \"../../Corpora/Normalized/RE03_critics.tsv\",\n",
    "            \"../../Corpora/Normalized/SM01_sb10k.tsv\",\n",
    "            \"../../Corpora/Normalized/SM02_potts.tsv\",\n",
    "            \"../../Corpora/Normalized/SM03_multiSe.tsv\",\n",
    "            \"../../Corpora/Normalized/SM04_gertwittersent.tsv\",\n",
    "            \"../../Corpora/Normalized/SM05_ironycorpus.tsv\",\n",
    "            \"../../Corpora/Normalized/SM06_celeb.tsv\"\n",
    "            ]\n",
    "    return file_ternary_raw, file_ternary, file_binary_raw, file_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractId(resultDf):\n",
    "    numberList=[]\n",
    "    for ids in range(len(resultDf[\"id\"])):\n",
    "        number=resultDf[\"id\"][ids]\n",
    "        numberList.append(number)\n",
    "    return numberList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractText(textDf, numberList):\n",
    "    allDataList=[]\n",
    "    for i in range(len(textDf[\"id\"])):\n",
    "        if textDf.iloc[i][\"id\"] not in numberList:\n",
    "            None\n",
    "        else:  \n",
    "            allDataList.append(textDf.iloc[i])\n",
    "            \n",
    "    return allDataList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertNewData(updateTextDf,resultDf):\n",
    "    textList=[]\n",
    "    flatTextList=[]\n",
    "    textList=updateTextDf.values.tolist()\n",
    "    for item in textList:\n",
    "        for subitem in item:\n",
    "            flatTextList.append(subitem)\n",
    "\n",
    "    resultDf[\"text\"]=flatTextList\n",
    "    return resultDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateDataframe(resultDf):\n",
    "    resultDf=resultDf.rename(columns={\"sentiment\":\"labels\"})\n",
    "    resultDf[\"labels\"].replace(to_replace=\"positive\", value=0, regex=True, inplace=True)\n",
    "    resultDf[\"labels\"].replace(to_replace=\"negative\", value=1, regex=True, inplace=True)\n",
    "    resultDf[\"labels\"].replace(to_replace=\"neutral\", value=2, regex=True, inplace=True)\n",
    "    return resultDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Index of List:  0\n",
      "Name of Corpus:  LT01_gnd_Preprocessed_ternary\n",
      "New Dataframe:        id  labels                                               text\n",
      "0      0       0  Ohne Zwang bist Du erwachsen, ohne Zwang gebli...\n",
      "1      1       1      Er reist ab, läßt zweitausend Pfund im Stich!\n",
      "2      2       1  Unzählbar ist die Menge aller Derer, deren Kra...\n",
      "3      3       1  Und beten heimlich zu Gott für die bedrängte K...\n",
      "4      4       1  Eine neue Prüfung hatte ich auszustehen, da me...\n",
      "..   ...     ...                                                ...\n",
      "265  265       0  Sie hatte sich da ganz geschickt benommen und ...\n",
      "266  266       1  Weil er aber kein  flüssiges Geld hatte, verpf...\n",
      "267  267       2  Auch mich laß eine Hülle über diese schrecklic...\n",
      "268  268       0  »Die Wunde heilt sehr gut – Sie werden kaum ei...\n",
      "269  269       2  Und man wird nun kommen und behaupten, daß ich...\n",
      "\n",
      "[270 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  1\n",
      "Name of Corpus:  LT02_speechLessing_Preprocessed_ternary\n",
      "New Dataframe:         id  labels                                               text\n",
      "0       2       1           ADRAST:Als ich dem Theophan, und er mir?\n",
      "1       3       0  ADRAST:Auch daran ist nicht gedacht worden. Wi...\n",
      "2       4       2                           ADRAST:Auf Ihr Anliegen?\n",
      "3       5       1        ADRAST:Das vortrefflich ungeheuer sein muß.\n",
      "4       8       1  ADRAST:Du Dummkopf! – Ja, er wird ihm zureden,...\n",
      "..    ...     ...                                                ...\n",
      "699  1175       1  WERNER:Potz Geck, und kein Ende! – Frauenzimme...\n",
      "700  1176       1  WERNER:Sie brauchen es nicht? Und verkaufen li...\n",
      "701  1177       2  WERNER:St! – hier sind hundert Dukaten, die ic...\n",
      "702  1178       1  WERNER:Versetzt! Glaub Sie doch so was nicht. ...\n",
      "703  1181       2  ZWEITER MAMELUCK:Es wird wohl noch ein dritter...\n",
      "\n",
      "[704 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  2\n",
      "Name of Corpus:  MI01_mlsa_Preprocessed_ternary\n",
      "New Dataframe:        id  labels                                               text\n",
      "0      0       0  Den Weltkrieg wertete er als gerechten Krieg ,...\n",
      "1      1       1  Eine verfestigte Neigung , auf beziehungs- bzw...\n",
      "2      2       2  Alle Menschen guten Willens müssen sich einset...\n",
      "3      3       2  In der Chemotherapie viraler Infektionen und z...\n",
      "4      4       1  Und doch wird in Israel über diese Möglichkeit...\n",
      "..   ...     ...                                                ...\n",
      "265  265       0  Vielleicht gibt das einen besseren Überblick ü...\n",
      "266  266       0              Ein wunderbarer Film , kein Zweifel !\n",
      "267  267       0  Einen massiven Aufschwung gewann sie durch die...\n",
      "268  268       0  Ehrfurcht erregender Stil , der den Neuling le...\n",
      "269  269       1  Andererseits würde ich mir wünschen , offen da...\n",
      "\n",
      "[270 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  3\n",
      "Name of Corpus:  MI02_germeval_Preprocessed_ternary\n",
      "New Dataframe:            id  labels                                               text\n",
      "0          0       2  @DB_Bahn ja, weil in Wuppertal Bauarbeiten sin...\n",
      "1          1       0  @nordschaf theoretisch kannste dir überall im ...\n",
      "2          2       1  Bahn verspätet sich..gleich kommt noch jemand ...\n",
      "3          3       2  Ihre Anfragen brachten uns zu neuen Leistungen...\n",
      "4          4       2  Kann ich mit dem DB Geschenk Ticket den ICE Sp...\n",
      "...      ...     ...                                                ...\n",
      "26675  26681       2  Re: Süddeutsche Zeitung Dieses Ergebnis hätte ...\n",
      "26676  26682       2  @jonahplank @MeinFernbus probier mal deutsche ...\n",
      "26677  26683       2  Neudorf ist ein teures Pflaster für Parkende ,...\n",
      "26678  26684       2  @extra3 Vielleicht mal angucken mit wem da gen...\n",
      "26679  26685       2  Züge fahren nicht bis Budapest Züge fahren nic...\n",
      "\n",
      "[26680 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  4\n",
      "Name of Corpus:  MI03_corpusRauh_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       2   voraussetzung dafür ist aber bis jetzt , daß ...\n",
      "1        1       2   es gibt aufgrund der ermittlung in den letzte...\n",
      "2        2       2   eltern mit schulpflichtigen kindern wissen , ...\n",
      "3        3       1                        das gegenteil ist der fall \n",
      "4        4       1        das kann ich ihnen nicht durchgehen lassen \n",
      "...    ...     ...                                                ...\n",
      "1420  1421       0   ich gehe davon aus , dass sie das auch tun we...\n",
      "1421  1422       1   haben sie nicht auch den eindruck , dass bei ...\n",
      "1422  1423       1   sind sie mit mir des weiteren der meinung , d...\n",
      "1423  1424       2   die bundesregierung wird über diese frage zu ...\n",
      "1424  1425       2   vom entwurf einer erklärung zur abstimmung üb...\n",
      "\n",
      "[1425 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  5\n",
      "Name of Corpus:  NA01_gersen_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       1  Das heißt, von Montag bis Mittwoch ist es dort...\n",
      "1        1       1  \"Die Mietpreisbremse hat mit uns nichts zu tun...\n",
      "2        2       1  Aufpassen sollte man bei Cumulonimbus: Damit s...\n",
      "3        3       1  Männer ließen Vorlesungen dagegen eher über si...\n",
      "4        4       1  Dazu gehören unter anderem die unverhältnismäß...\n",
      "...    ...     ...                                                ...\n",
      "2329  2329       0  Bürgermeister Markus Günther war erfreut, dass...\n",
      "2330  2330       0  ﻿Wissenschaft – Spitzenforschung ebenso wie ex...\n",
      "2331  2331       0  Man gibt einfach im Navi ein Ziel ein und läss...\n",
      "2332  2332       0  Er wurde mit der DECHEMA-Plakette in Titan, de...\n",
      "2333  2333       0  Durch sein Engagement hat er ein neues, politi...\n",
      "\n",
      "[2334 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  6\n",
      "Name of Corpus:  NA02_gerom_Preprocessed_ternary\n",
      "New Dataframe:        id  labels                                               text\n",
      "0      0       0  „Es war das großartigste Erlebnis unseres Lebens“\n",
      "1      1       0               «Darüber habe ich mich sehr gefreut»\n",
      "2      2       0  „Es ist positiv, dass an die Zeit erinnert wird.“\n",
      "3      3       0  «Ich bin das erste Jahr als Trainer in der Bun...\n",
      "4      4       0  \"Boateng ist ein sehr guter Spieler und wird e...\n",
      "..   ...     ...                                                ...\n",
      "846  846       2  Assad die Waffenruhe erst in einer Woche umset...\n",
      "847  847       2  „Ich habe durch meine Stiftung und den Job als...\n",
      "848  848       2  „Das Besondere an unserer Schule ist, dass die...\n",
      "849  849       2  \"anders als ehrenhaft\" entlassen und die meist...\n",
      "850  850       2  „Die Kombination aus Theorie und Praxis ist op...\n",
      "\n",
      "[851 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  7\n",
      "Name of Corpus:  NA03_ompc_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       1  !Verdachtsdiagnose! - weil ich die beschrieben...\n",
      "1        1       1  \" saying that securing the Schengen area’s out...\n",
      "2        2       1  \"... Idomeni: Polizei setzte erneut Tränengas ...\n",
      "3        3       1  \"... mit den bisherigen sind wir wirtschaftlic...\n",
      "4        4       1  \"... oder wegen dieser Befürchtungen nicht in ...\n",
      "...    ...     ...                                                ...\n",
      "3396  3434       2  “This is not the IDF, these are not the values...\n",
      "3397  3435       2  „Wo Unrecht zu Recht wird, wird Widerstand zur...\n",
      "3398  3436       2  …die sich nicht NOCH EINMAL vorschreiben lasse...\n",
      "3399  3437       2  …es natürlich psych_i_atrie heißen muss,…\\r\\n\\...\n",
      "3400  3438       1  …hoffentlich bekommt er bald Arbeitslosengeld ...\n",
      "\n",
      "[3401 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  8\n",
      "Name of Corpus:  RE01_usage_Preprocessed_ternary\n",
      "New Dataframe:        id  labels                                               text\n",
      "0      0       0            Der Mülleimer war wirklich sehr einf...\n",
      "1      1       0            Dieses Modell hat bei uns den Vorgän...\n",
      "2      2       0            Für den Preis ist das Produkt völlig...\n",
      "3      3       0            Einfach praktisch, laufruhig passt g...\n",
      "4      4       0            Habe den Mülleimer jetzt schon ewig ...\n",
      "..   ...     ...                                                ...\n",
      "585  606       0            Die Kaffeemaschine sieht nicht nur t...\n",
      "586  607       0            Nachdem man den Toaster ausgepackt h...\n",
      "587  608       0            supper schnell mit tolle OR.verpacku...\n",
      "588  609       0            Der Toaster ist super, sieht gut aus...\n",
      "589  610       0            Die Farbe des Toasters ist wie auf d...\n",
      "\n",
      "[590 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  9\n",
      "Name of Corpus:  RE03_critics_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       1  Anzengruber: Bei alledem bleibt doch auch für ...\n",
      "1        1       1  Anzengruber: Die letzten Stücke Anzengruber’s,...\n",
      "2        2       1  Anzengruber: In dieser Erzählung passirt Roseg...\n",
      "3        3       1  Anzengrubers: Doch immerhin bleibt das Bedauer...\n",
      "4        4       1  Anzengrubers: [...] Auch mit der Einschleppung...\n",
      "...    ...     ...                                                ...\n",
      "1680  1682       0  Zola: Wenn aber ein Kunstwerk im besten Sinne ...\n",
      "1681  1683       0  Zola: Wir verbleiben, auch in der Welt des Tra...\n",
      "1682  1684       0  Zola: Zola schildert ein Geschlecht, das sich ...\n",
      "1683  1685       0  Zola: Zunächst wird Zola, sowohl was die künst...\n",
      "1684  1686       0  Zolas: Das ist das Prinzip aller echten Poesie...\n",
      "\n",
      "[1685 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  10\n",
      "Name of Corpus:  SM01_sb10k_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       0  RT @TheKedosZone: So ein Hearthstone-Key von @...\n",
      "1        1       2  Tainted Talents (Ateliertagebuch.) » Wir sind ...\n",
      "2        2       2  Aber wenigstens kommt #Supernatural heute mal ...\n",
      "3        3       2  DARLEHEN - Angebot für Schufa-freie Darlehen: ...\n",
      "4        4       2  ANRUF ERWÜNSCHT: Hardcore Teeny Vicky Carrera:...\n",
      "...    ...     ...                                                ...\n",
      "7423  7471       2  CL-Qualifikation: Zenit mit Kantersieg, Austri...\n",
      "7424  7472       1  @AstridSchiebs Okay. @DerAtrox Machse einfach!...\n",
      "7425  7473       2  Öffentliche Pfeifturmbegehung Führung um 11:30...\n",
      "7426  7474       0  Ich muss gerade irgendwem, der mich nicht dafü...\n",
      "7427  7475       0  @FrauFlauschig Ich mag ihn auch echt gerne, ha...\n",
      "\n",
      "[7428 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  11\n",
      "Name of Corpus:  SM02_potts_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       0  @HendricRuesch @tiniee76 für die @GalaLive rei...\n",
      "1        1       0  RT @DaBiebahConvers : @DaBieberVans ja mach da...\n",
      "2        2       0  Rating - Update bei Cache me if you can ! :-) ...\n",
      "3        3       0  Omg ich komm von der Schule und sehe dass @Lik...\n",
      "4        4       0  ein tag voller adrenalin und kalorien :) http:...\n",
      "...    ...     ...                                                ...\n",
      "7289  7767       1         Sieht ja nich mehr so frisch aus xO #papst\n",
      "7290  7768       0  Wird hier hetzt von jedem ein papst - tweet er...\n",
      "7291  7769       1  Irgendwie bizzarr , wenn im Stadion News über ...\n",
      "7292  7770       0  RT @SatireLupe : Welches Gras rauchen eigentli...\n",
      "7293  7771       0  RT @SPIEGELONLINE : #Papst Francisco nahm sich...\n",
      "\n",
      "[7294 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  12\n",
      "Name of Corpus:  SM03_multiSe_Preprocessed_ternary\n",
      "New Dataframe:          id  labels                                               text\n",
      "0        0       2  New Post:  Samsung bestätigt Produktionsbeginn...\n",
      "1        1       2  Base Lutea oder Samsung Galaxy Ace?  Ich hab d...\n",
      "2        2       2  Jeden Tag ein neues Aktionsangebot - das Kenne...\n",
      "3        3       2  Hör dir Katy Perry von Firework an! Ich habe d...\n",
      "4        4       0  Dank @user geht es mir jetzt schon besser *_* ...\n",
      "...    ...     ...                                                ...\n",
      "1653  1655       0  Mit einem schnellen; gekonnten Schlag zerbrach...\n",
      "1654  1656       2  Betäubung hält 4 Stunden . Ich hab jetzt schon...\n",
      "1655  1657       2  Jetzt heißt es endspurt beim @user - denn heut...\n",
      "1656  1658       1  Der Abzug des Holzkohlegrill in seiner vollen ...\n",
      "1657  1659       2                   Buchon : persona de buche grande\n",
      "\n",
      "[1658 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  13\n",
      "Name of Corpus:  SM04_gertwittersent_Preprocessed_ternary\n",
      "New Dataframe:            id  labels                                               text\n",
      "0          0       1  @TuT_Parody so einen Rasen hätte sich Hoeneß d...\n",
      "1          1       2  RT @heisec: Apples iCloud verschickt und empfä...\n",
      "2          2       0                      @kopfding Pass bloß auf!!! ;)\n",
      "3          3       1  @kopfding @Stephan535 Es gibt nichts antikeres...\n",
      "4          4       1  Abstiegsangst! - Kind will mit Korkut sprechen...\n",
      "...      ...     ...                                                ...\n",
      "64496  64632       2  Willst Du Dein Profilbild adaptieren und auf #...\n",
      "64497  64633       2  RT @FAZ_Finanzen: Apple tauscht Netzteile älte...\n",
      "64498  64634       2  Gut das wir nie auf Butter verzichtet haben un...\n",
      "64499  64635       0  3/3 Gratulation @LucasSobotka für den Weitblic...\n",
      "64500  64636       2  Tolle Grafik über den Twitter-Traffic während ...\n",
      "\n",
      "[64501 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  14\n",
      "Name of Corpus:  SM05_ironycorpus_Preprocessed_ternary\n",
      "New Dataframe:        id  labels                                               text\n",
      "0      0       1  Muss sich der @DFB nach den Pyros bei #sgebvb ...\n",
      "1      1       1  @DFB_Team gratuliert Arsenal, aber nicht der B...\n",
      "2      2       1  #SGEBVB #HeleneFischer Hätte nie gedacht, dass...\n",
      "3      3       1  Auch mit Holzhammer-Reimen wie \"Leben\" auf \"-l...\n",
      "4      4       1  Wenn Helene Fischers Auftritt ein Symbol der K...\n",
      "..   ...     ...                                                ...\n",
      "158  158       0  Ok, er ist ein Nimmersatt! Nein, nicht nur @Cr...\n",
      "159  159       1  Krass, wie die ARD die Stadionmikros runterdre...\n",
      "160  160       1  So laut haben sich die Fans nicht mal bei der ...\n",
      "161  161       1  Wie das Playback nicht mal die Pfiffe übertönt...\n",
      "162  162       1  Eins ist klar, solche Halbzeitshows wie von He...\n",
      "\n",
      "[163 rows x 3 columns]\n",
      "-------------------------\n",
      "Start\n",
      "Index of List:  15\n",
      "Name of Corpus:  SM06_celeb_Preprocessed_ternary\n",
      "New Dataframe:        id  labels                                               text\n",
      "0      0       0  UDO! Du bist unser Held! Wann kommt eine DVD v...\n",
      "1      1       0             Das war schon vor Jahren mein Lied!!!!\n",
      "2      2       0  Super geil der alte Haudegen wie in alten Zeit...\n",
      "3      3       0  Udo,du bist der schärfste auf diesem Planeten ...\n",
      "4      5       0  läuft seit Donnerstag dank Amazon bei mir im P...\n",
      "..   ...     ...                                                ...\n",
      "485  495       0  Nina ist und bleibt eine der größten Künstler ...\n",
      "486  496       0                   Man muß die Süße einfach lieben.\n",
      "487  497       0  Die Dame ist eben ein wenig....hmm ich sag mal...\n",
      "488  498       1         Die Frau ist aber alles andere als seriös.\n",
      "489  499       0  Als Künstlerin ist sie mindestens so seriös wi...\n",
      "\n",
      "[490 rows x 3 columns]\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    loadingData=loadFiles()\n",
    "    file_ternary_raw=loadingData[0]\n",
    "    file_ternary=loadingData[1]\n",
    "    file_binary_raw=loadingData[2]\n",
    "    file_binary=loadingData[3]\n",
    "    for index in range(len(file_ternary_raw)):\n",
    "        print(\"Start\")\n",
    "        print(\"Index of List: \",index)\n",
    "            \n",
    "        file = file_ternary[index]\n",
    "        fileText = file_ternary_raw[index]\n",
    "            \n",
    "        path,fileName =os.path.split(file)\n",
    "        fileName= re.sub(\".tsv\",\"\",fileName)\n",
    "            \n",
    "        print(\"Name of Corpus: \", fileName)\n",
    "\n",
    "        idDf = pd.read_csv(file , sep=\"\\t\")\n",
    "        textDf=pd.read_csv(fileText,sep=\"\\t\")\n",
    "\n",
    "        resultDf= pd.DataFrame()\n",
    "            \n",
    "        resultDf=idDf.filter([\"id\",\"sentiment\"])\n",
    "        textDf=textDf.filter([\"id\",\"text\",\"sentiment\"])\n",
    "\n",
    "        numberList=extractId(resultDf)\n",
    "        allDataList=extractText(textDf,numberList)\n",
    "\n",
    "        updateTextDf=pd.DataFrame(allDataList,columns=[\"id\",\"text\",\"sentiment\"])\n",
    "        updateTextDf=updateTextDf.filter([\"text\"])\n",
    "            \n",
    "        resultDf=insertNewData(updateTextDf,resultDf)\n",
    "        resultDf=updateDataframe(resultDf)\n",
    "            \n",
    "        print(\"New Dataframe: \",resultDf)\n",
    "        print(\"-------------------------\")\n",
    "            \n",
    "        newFileName=fileName+\"_Transformer.tsv\"\n",
    "        resultDf.to_csv(newFileName, sep=\"\\t\",index=False)\n",
    "main()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
